---
layout: default
---


[/index](./)
## Computer Chronicles

The history of computers begins with the abacus, which first appeared in the East more than 5,000 years ago. The abacus is constructed so that calculations can be performed by sliding counters — balls — along parallel bars. The counters are divided into two fields with a partition perpendicular to the bars. One field has one or two counters, corresponding to the numbers 1 and 5 depending on their position on the bar. The second field has four or five counters, corresponding to units. Each partition corresponds to an important digit, with the least important one on the right (or bottom) side.

Mechanical computers were invented in Europe in the 17th century. The first such device was a cumulative machine built in 1624 by the French scientist and philosopher Blaise Pascal as a computing aid for keeping track of his father's accounts. This device was a sequence of wheels, the positions of which could be observed through windows arranged in the housing of the mechanism. There were selection wheels for entering the numbers. Pascal's machine was a digital device that performed its calculations using the integer enumeration process. It used a mechanical gear system for additions and subtractions, with up to eight digit columns.

German mathematician Gottfried Wilhelm Leibniz saw Pascal's machine in Paris and developed a more advanced variety. While Pascal's device had only a counting capacity, Leibniz's selected Schritt Rechner could still multiply, divide, and extract square roots. An operating model of the machine was completed in 1673 and presented to the Royal Society in London. The first commercially available Tom de Colmar numerator was built in 1820, but like Leibniz's invention, it proved unreliable.

The first automatic digital computer was invented in the 1830s by the English inventor Charles Babbage. This mechanical device, called the Analytical Engine, was intended to combine arithmetic operations with decisions based on its calculations. Babbage's designs incorporated most of the basic components of the modern digital computer, such as a card input / output medium, arithmetic unit, number storage memory, and sequence control. The Analytical Engine was never completed, mainly because precision techniques had not yet been developed for the manufacture of low tolerance metal components. Babagge's invention was forgotten until his writings were rediscovered in 1937.

More important for the evolution of the digital computer was the work of the English mathematician and philosopher of logic George Boole. In various essays he wrote in the 19th century, Boole drew attention to the analogy between the symbols of algebra and those of logic as they are used to represent logical patterns and reasoning. The Boole system, with its binary logical operators (eg AND, OR, NOR, etc.) formed the basis of what is now known as Boole algebra, on which the theory and processes of computer switching are based.

Another important step in the development of computers was the introduction of perforated cards in data processing by the American statistician Herman Hollerith. Working on the results of the 1880 census, Hollerith realized that a design of holes in perforated cards could be electrically identified by a machine specially designed to classify and manage the numerical data represented by the holes. It is very likely that he was inspired by the work of the Frenchman Joseph-Marie Jacquard, who in the early 19th century invented a method of controlling the operation of the loom through perforated cards. During the American Census of 1890, Hollerith had invented a tabulation system that automated inventory counts. The electromechanical reading and drilling devices included in the Hollerith system were the forerunners of modern computer peripherals.

In 1939, the American mathematician and physicist John Vincent Atanasoff built a laboratory prototype of an electromechanical digital computer, a feature of which was the first application of electronic vacuum tubes for calculations. That same year, Howard Aiken of Harvard University, in collaboration with IBM engineers, began work on developing a fully-automatic large-scale computer using standard office machine components. In 1944 they completed the Automatic Sequence Controller, commonly known as the Harvard Mark I. It was a huge electromechanical machine, about 15 meters long and 2.4 meters high, whose functions were controlled by a sequence of commands encoded on perforated paper. The data was entered through perforated cards and the results were recorded either on identical cards or with an electric typewriter. The input data was processed by relays (electrical switching devices).

Since the advent of Mark I, the digital computer has evolved rapidly. The sequence of advances in the hardware part of computers, especially in logic circuits and storage systems, is examined below with the terminology of the concept of generations, where the machines of each generation of computers use common technology.

### first generation

The J. Presper Eckert and J. W. Mauchly, both of the University of Pennsylvania, launched the first generation of modern computers with ENIAC (Electronic Numerical Integrator and Calculator). ENIAC was completed in 1946 and was the first general-purpose, fully electronic digital computer. A special-purpose, fully electronic computer called the Colossus had been developed earlier in Bletchley Park, north London, and had been in operation since December 1943. The Colossus was intended to decrypt codes generated by German electromechanical devices known as Enigma and Geheim Schreiber (cryptographer). Many different patterns were used until the end of World War II. Both the ENIAC and the Colossus series of machines used vacuum tubes instead of relays as their active logic components, which led to a significant increase in computing speed. ENIAC was 1,000 times faster than its electromechanical precursors and could perform up to 5,000 basic arithmetic operations per second.

ENIAC was an externally programmable machine that used plug sockets on which the sequence of operations required to solve each individual problem could be arranged. His successor at the University of Pennsylvania was the EDVAC (Electronic Discrete Variable Automatic Computer), a machine of radically different design influenced by ideas incubated at Princeton University's Institute for Advanced Study. EDVAC was a stored program computer in which commands were handled in the same way as numeric data and stored electronically in the computer memory. The beginning of the memorized program led to the development of automated computer programs, as the numerical capabilities of the machine allowed the manipulation of commands with the same ease as handling numerical data.

EDVAC was not completed until 1950, but a computer with a similar design, the EDSAC (Electronic Delay Storage Automatic Computer) at the University of England, became operational in early 1949 and was probably the first operating model of a memorized program. Also in the mid-1940s, Hungarian-born mathematician John von Neumann, along with Herman H. Goldstine, presented a paper describing the idea of the memorized program and its incorporation into the study of computer development at the Institute for Advanced Study. , the IAS computer. Several early computers, such as the ORDVAC (Ordinance Variable Automatic Computer), Whirlwind I, and IBM 701, were designed in the directions suggested in this paper.

Another notable first-generation digital computer was the UNIVAC I (Universal Automatic Computer), built in 1951 by Eckert and Mauchly for the US Census Bureau. Like all computers of this generation, UNIVAC I used vacuum tubes —  mainly triode and pentode. His main memory consisted of lines of mercury delay. In each such delay line a mercury lamp served as an acoustic medium, through which sound pulses representing bits 0 and 1 were transmitted over a specified distance between a pair of transducers. This memory system allowed an access time of 500 μsec. UNIVAC I was the first digital computer to handle arithmetic and alphabetical information with equal ease and used the principle of separation of input / output from computing itself.

Although the logic technology of first-generation computers remained essentially the same as that of UNIVAC I, it had improved in main memory. Harvard Mark III, for example, had a magnetic drum memory, which offered a relatively large storage capacity but slow access of about 17 msec. By the mid-1950s, reliable magnetic core memory modules had been developed that were integrated into machines such as UNIVAC II.

### second generation

In 1959, machines using the semiconductor components known as transistors were introduced to the market. The transistor was essentially invented in 1947, but it took more than ten years of evolutionary work to make it a viable replacement for the vacuum tube. The small size of the transistor, its greater reliability and the comparatively low power consumption made it much superior to its predecessor. By using transistors in control circuits, arithmetic operations, and logic circuits, combined with an improved magnetic core memory, computer manufacturers have been able to produce more efficient, smaller, and faster (up to 100,000 commands per second) digital systems.

### third generation

In the late 1960s and 1970s the electronic components shrank even further, causing spectacular advances in the hardware of computers. The first major innovation was the construction of the integrated circuit (IC), a solid structure device consisting of hundreds of transistors, diodes and resistors mounted on a tiny silicon chip. The use of ICs allowed the construction of large (mainframe) computers that offered higher operating speeds, higher capacity and greater reliability at a lower cost. Their location also allowed engineers to design minicomputers, machines small enough to fit on a desk and powerful enough to control the operation of a small factory or to supervise the instruments of a science lab.

The next major development was the large scale integration (LSI), which allowed the accumulation of thousands of transistors and related devices in a single integrated circuit. This microcircuit technique produced two devices that revolutionized computer technology. The first of these devices was the microprocessor. Often referred to as the computer on a chip, the microprocessor is an integrated circuit containing all the numerical, logic, and control circuits required to operate as a central processing unit (CPU) — the part of the digital computer that interprets and executes commands. Its development has led to the production of microcomputers (systems no larger than a portable TV set) and smart peripherals.

As the number of circuits that could be integrated into a single chip increased, so did the need for different technologies for logic and memory, resulting in semiconductor memories gradually replacing magnetic core memory in new computer systems. The random access memory (RAM) chip used in the construction of semiconductor memory modules is the second major device to emerge from LSI technology.

### fourth generation

The computers of the 1980s are often referred to as fourth generation systems although the differences between them and third generation systems are not always clear. Most of their distinctive features come from very large scale integration (VLSI) applications. This technology has greatly increased the circuit density of the microprocessor, memory and support chips (those used to communicate the microprocessors with the input / output devices). While large-scale integrated circuits contain thousands of components in a silicon chip with a surface area of less than 5x5 mm / mm, large-scale integrated circuits contain hundreds of thousands of components in the same space. The cost reduction associated with these developments has made it possible to build simple computers for home and school use.

The abundance of reliable and inexpensive computer systems and the consequent advances in programming have led to the development of computer networks. These networks are collections of standalone PC peripherals (e.g. ATMs, point-of-sale terminals equipped with microprocessors) interconnected via telephone lines, microwave transponders, and other high-speed communications connectors typically installed with use and exchange of resources or data. Networking has been developed at all levels, from local to international, in various sectors of society.

Since the early 1980s, many computer scientists and engineers, especially in Japan and the United States, have stepped up their efforts to develop computer systems endowed with what is often called artificial intelligence (AI). Such machines, the selected fifth generation, have the ability to judge, recognize relationships and learn. They basically have the ability to improve their performance based on experience. These machines have an internal architecture that allows parallel processing — that is, the simultaneous execution of many distinct functions (e.g. memory, logic and control) through numerous integrated circuits in which millions of CPU, memory and input / output circuits work together. Virtually all computers of this generation perform their functions serially or sequentially: separate input circuits supply data to individual memory cells, each of which transmits the information bit one by one to a single CPU for processing, the results of which are then transmitted to an external discharge device. Although today's faster computers — called supercomputers — can execute around 50,000,000 commands per second, they are still too slow to reach higher forms of human thinking that require rapid correlations and generalizations.

Significant software improvements, such as native language programs, also help to establish the premise for smart computers. Programs like this allow computers to understand written commands in common English. Single-chip signal processors are also available that can be programmed to recognize simple words and / or entire spoken sentences. Another development is the knowledge systems, based on knowledge, which require the storage and application of huge amounts of specialized knowledge in special complex, non-numerical problems. This type of program has been widely used as an aid to physicians in diagnosing and prescribing treatment. More important developments for artificial intelligence, more for research, are the efforts to develop computer programs that can produce other programs. Such software is capable of transforming the art of programming if not eliminating it altogether (see [AlphaCode in DeepMind](https://www.theverge.com/2022/2/2/22914085/alphacode-ai-coding-program-automatic-deepmind-codeforce)).

